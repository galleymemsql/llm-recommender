{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Recommender"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create a workpace in your workspace group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create a database in the workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install and import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:15.978807Z",
          "iopub.status.busy": "2024-01-10T18:29:15.978565Z",
          "iopub.status.idle": "2024-01-10T18:29:17.900211Z",
          "shell.execute_reply": "2024-01-10T18:29:17.899595Z",
          "shell.execute_reply.started": "2024-01-10T18:29:15.978792Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%pip install singlestoredb openai tiktoken beautifulsoup4 pandas python-dotenv Markdown praw tweepy --quiet\n",
        "\n",
        "import re\n",
        "import json\n",
        "import openai\n",
        "import tiktoken\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import singlestoredb as s2\n",
        "import tweepy\n",
        "import praw\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from datetime import datetime\n",
        "from time import time, sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Set variables\n",
        "\n",
        "### 4.1. Set the app common variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODELS_LIMIT = 300\n",
        "MODELS_TABLE_NAME = 'models'\n",
        "MODEL_READMES_TABLE_NAME = 'model_readmes'\n",
        "MODEL_TWITTER_POSTS_TABLE_NAME = 'model_twitter_posts'\n",
        "MODEL_REDDIT_POSTS_TABLE_NAME = 'model_reddit_posts'\n",
        "MODEL_GITHUB_REPOS_TABLE_NAME = 'model_github_repos'\n",
        "LEADERBOARD_DATASET_URL = 'https://llm-recommender.vercel.app/datasets/leaderboard.json'\n",
        "TOKENS_LIMIT = 2047\n",
        "TOKENS_TRASHHOLD_LIMIT = TOKENS_LIMIT - 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Set the OpenAI variables\n",
        "1. [Open the OpenAI API keys page](https://platform.openai.com/api-keys)\n",
        "2. Create a new key\n",
        "3. Copy the key and paste it into the `OPENAI_API_KEY` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3. Set the HuggingFace variables\n",
        "1. [Open the HuggingFace Access Tokens page](https://huggingface.co/settings/tokens)\n",
        "2. Create a new token\n",
        "3. Copy the key and paste it into the `HF_TOKEN` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HF_TOKEN = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4. Set the Reddit variables\n",
        "\n",
        "1. [Open the Reddit Apps page](https://www.reddit.com/prefs/apps)\n",
        "2. Click on the `Create another app` button\n",
        "3. Fill the form:\n",
        "    - `name` - name the app as you wish\n",
        "    - `redirect uri` - enter any http address, for example: http://localhost:4000\n",
        "4. Click on the `Create app` button\n",
        "5. Copy the `personal use script` value (it is located right below the app name) and paste it into the `REDDIT_CLIENT_ID` variable\n",
        "6. Copy the `secret` value and paste it into the `REDDIT_CLIENT_SECRET` variable\n",
        "7. Enter values into the remaining variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REDDIT_USERNAME = ''\n",
        "REDDIT_PASSWORD = ''\n",
        "REDDIT_CLIENT_ID = ''\n",
        "REDDIT_CLIENT_SECRET = ''\n",
        "REDDIT_USER_AGENT = 'llm_recommender_1.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5. Set the Twitter variables\n",
        "1. [Open the Twitter Developer Projects & Apps page](https://developer.twitter.com/en/portal/projects-and-apps)\n",
        "2. Add a new app\n",
        "3. Fill the form\n",
        "4. Generate a Bearer Token and paste it into the `TWITTER_BEARER_TOKEN` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TWITTER_BEARER_TOKEN = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6. Set the GitHub variables\n",
        "1. [Open the Register new GitHub App page](https://github.com/settings/apps/new)\n",
        "2. Fill the form\n",
        "3. Get an access token and paste it into the `GITHUB_ACCESS_TOKEN` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GITHUB_ACCESS_TOKEN = ''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create a database connection and functions\n",
        "\n",
        "- `connection` - database connection to execute queries\n",
        "- `create_tables` - function that creates empty tables in the database\n",
        "- `drop_table` - helper function to drop a table\n",
        "- `get_models` - helper function to get models from the models table\n",
        "- `db_get_last_created_at` - helper function to get last `created_at` value from a table\n",
        "\n",
        "The `create_tables` creates the following tables:\n",
        "- `models_table` - table with all models data from the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
        "- `readmes_table` - table with model readme texts from the HugginFace model pages (used in semantic search)\n",
        "- `twitter_posts` - table with tweets related to models (used in semantic search)\n",
        "- `reddit_posts` - table with Reddit posts related to models (used in semantic search)\n",
        "- `github_repos` - table with GitHub readme texts related to models (used in semantic search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:17.915576Z",
          "iopub.status.busy": "2024-01-10T18:29:17.915203Z",
          "iopub.status.idle": "2024-01-10T18:29:18.388970Z",
          "shell.execute_reply": "2024-01-10T18:29:18.388047Z",
          "shell.execute_reply.started": "2024-01-10T18:29:17.915558Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "connection = s2.connect(connection_url)\n",
        "\n",
        "\n",
        "def create_tables():\n",
        "    def create_models_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODELS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    name VARCHAR(512) NOT NULL,\n",
        "                    author VARCHAR(512) NOT NULL,\n",
        "                    repo_id VARCHAR(1024) NOT NULL,\n",
        "                    score DECIMAL(5, 2) NOT NULL,\n",
        "                    arc DECIMAL(5, 2) NOT NULL,\n",
        "                    hellaswag DECIMAL(5, 2) NOT NULL,\n",
        "                    mmlu DECIMAL(5, 2) NOT NULL,\n",
        "                    truthfulqa DECIMAL(5, 2) NOT NULL,\n",
        "                    winogrande DECIMAL(5, 2) NOT NULL,\n",
        "                    gsm8k DECIMAL(5, 2) NOT NULL,\n",
        "                    link VARCHAR(255) NOT NULL,\n",
        "                    downloads INT,\n",
        "                    likes INT,\n",
        "                    still_on_hub BOOLEAN NOT NULL,\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_readmes_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_READMES_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_twitter_posts_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_TWITTER_POSTS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    post_id VARCHAR(256),\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_reddit_posts_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_REDDIT_POSTS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    post_id VARCHAR(256),\n",
        "                    title VARCHAR(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    link VARCHAR(256),\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_github_repos_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_GITHUB_REPOS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    repo_id INT,\n",
        "                    name VARCHAR(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    description TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    link VARCHAR(256),\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    create_models_table()\n",
        "    create_model_readmes_table()\n",
        "    create_model_twitter_posts_table()\n",
        "    create_model_reddit_posts_table()\n",
        "    create_model_github_repos_table()\n",
        "\n",
        "\n",
        "def drop_table(table_name: str):\n",
        "    with connection.cursor() as cursor:\n",
        "        cursor.execute(f'DROP TABLE IF EXISTS {table_name}')\n",
        "\n",
        "\n",
        "def get_models(select='*', query='', as_dict=True):\n",
        "    with connection.cursor() as cursor:\n",
        "        _query = f'SELECT {select} FROM {MODELS_TABLE_NAME}'\n",
        "\n",
        "        if query:\n",
        "            _query += f' {query}'\n",
        "\n",
        "        cursor.execute(_query)\n",
        "\n",
        "        if as_dict:\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
        "\n",
        "        return cursor.fetchall()\n",
        "\n",
        "\n",
        "def db_get_last_created_at(table, repo_id, to_string=False):\n",
        "    with connection.cursor() as cursor:\n",
        "        cursor.execute(f\"\"\"\n",
        "            SELECT UNIX_TIMESTAMP(created_at) FROM {table}\n",
        "            WHERE model_repo_id = '{repo_id}'\n",
        "            ORDER BY created_at DESC\n",
        "            LIMIT 1\n",
        "        \"\"\")\n",
        "\n",
        "        rows = cursor.fetchone()\n",
        "        created_at = float(rows[0]) if rows and rows[0] else None\n",
        "\n",
        "        if (created_at and to_string):\n",
        "            created_at = datetime.fromtimestamp(created_at)\n",
        "            created_at = created_at.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "        return created_at"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Set the AI API key and create AI functions\n",
        "\n",
        "### 6.1. Assing the `openai.api_key`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2. Create the `count_tokens` function\n",
        "This function used to count text tokens. It is used when a long text needs to be broken into chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_tokens(text: str):\n",
        "    enc = tiktoken.get_encoding('cl100k_base')\n",
        "    return len(enc.encode(text, disallowed_special={}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3. Create the `create_embedding` function\n",
        "This function used to create an embedding based on an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.390609Z",
          "iopub.status.busy": "2024-01-10T18:29:18.390411Z",
          "iopub.status.idle": "2024-01-10T18:29:18.398027Z",
          "shell.execute_reply": "2024-01-10T18:29:18.397575Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.390592Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_embedding(input):\n",
        "    try:\n",
        "        data = openai.embeddings.create(input=input, model='text-embedding-ada-002').data\n",
        "        return data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return [[]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Create the `JSONEncoder` class\n",
        "This class helps to convert datetime into the right JSON format "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class JSONEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, datetime):\n",
        "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        return super().default(obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2. Create the `list_into_chunks` function\n",
        "This function splits a list into the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_into_chunks(lst, chunk_size=100):\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3. Create the `string_into_chunks` function\n",
        "This function splits a long text into the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def string_into_chunks(string: str, max_tokens=TOKENS_LIMIT):\n",
        "    if count_tokens(string) <= max_tokens:\n",
        "        return [string]\n",
        "\n",
        "    delimiter = ' '\n",
        "    words = string.split(delimiter)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        if count_tokens(delimiter.join(current_chunk + [word])) <= max_tokens:\n",
        "            current_chunk.append(word)\n",
        "        else:\n",
        "            chunks.append(delimiter.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(delimiter.join(current_chunk))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4. Create the `clean_string` function\n",
        "This function removes all html and markdown elements from a string. This function is required when the number of characters needs to be reduced before converting to embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.399019Z",
          "iopub.status.busy": "2024-01-10T18:29:18.398833Z",
          "iopub.status.idle": "2024-01-10T18:29:18.405786Z",
          "shell.execute_reply": "2024-01-10T18:29:18.405275Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.399001Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def clean_string(string: str):\n",
        "    def strip_html_elements(string: str):\n",
        "        html = markdown(string)\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = soup.get_text()\n",
        "        return text.strip()\n",
        "\n",
        "    def remove_unicode_escapes(string: str):\n",
        "        return re.sub(r'[^\\x00-\\x7F]+', '', string)\n",
        "\n",
        "    def remove_string_spaces(strgin: str):\n",
        "        new_string = re.sub(r'\\n+', '\\n', strgin)\n",
        "        new_string = re.sub(r'\\s+', ' ', new_string)\n",
        "        return new_string\n",
        "\n",
        "    def remove_links(string: str):\n",
        "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        return re.sub(url_pattern, '', string)\n",
        "\n",
        "    new_string = strip_html_elements(string)\n",
        "    new_string = remove_unicode_escapes(new_string)\n",
        "    new_string = remove_string_spaces(new_string)\n",
        "    new_string = re.sub(r'\\*\\*+', '*', new_string)\n",
        "    new_string = re.sub(r'--+', '-', new_string)\n",
        "    new_string = re.sub(r'====+', '=', new_string)\n",
        "    new_string = remove_links(new_string)\n",
        "\n",
        "    return new_string"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create Leaderboard functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1. Create the `leaderboard_get_df` function\n",
        "This function loads a pre-generated Open LLM Leaderboard dataset. Based on this dataset, all model data is created and inserted into the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def leaderboard_get_df():\n",
        "    response = requests.get(LEADERBOARD_DATASET_URL)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = json.loads(response.text)\n",
        "        df = pd.DataFrame(data).head(MODELS_LIMIT)\n",
        "        return df\n",
        "    else:\n",
        "        print(\"Failed to retrieve JSON file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2. Create the `leaderboard_insert_model` function\n",
        "This function prepares a model, creates embedding based on the LLM Leaderboard model data and inserts into the `models` and `model_readmes` tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def leaderboard_insert_model(model):\n",
        "    try:\n",
        "        _model = {key: value for key, value in model.items() if key != 'readme'}\n",
        "        to_embedding = json.dumps(_model, cls=JSONEncoder)\n",
        "        embedding = str(create_embedding(to_embedding))\n",
        "        model_to_insert = {**_model, embedding: embedding}\n",
        "        readmes_to_insert = []\n",
        "\n",
        "        if model['readme']:\n",
        "            readme = {\n",
        "                'model_repo_id': model['repo_id'],\n",
        "                'text': model['readme'],\n",
        "                'created_at': time()\n",
        "            }\n",
        "\n",
        "            if count_tokens(readme['text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
        "                readme['clean_text'] = clean_string(readme['text'])\n",
        "                to_embedding = json.dumps({\n",
        "                    'model_repo_id': readme['model_repo_id'],\n",
        "                    'clean_text': readme['clean_text'],\n",
        "                })\n",
        "                readme['embedding'] = str(create_embedding(to_embedding))\n",
        "                readmes_to_insert.append(readme)\n",
        "            else:\n",
        "                for i, chunk in enumerate(string_into_chunks(readme['text'])):\n",
        "                    _readme = {\n",
        "                        **readme,\n",
        "                        'text': chunk,\n",
        "                        'created_at': time()\n",
        "                    }\n",
        "\n",
        "                    _readme['clean_text'] = clean_string(chunk)\n",
        "                    to_embedding = json.dumps({\n",
        "                        'model_repo_id': _readme['model_repo_id'],\n",
        "                        'clean_text': chunk,\n",
        "                    })\n",
        "                    _readme['embedding'] = str(create_embedding(to_embedding))\n",
        "                    readmes_to_insert.append(_readme)\n",
        "\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                INSERT INTO {MODELS_TABLE_NAME} (name, author, repo_id, score, link, still_on_hub, arc, hellaswag, mmlu, truthfulqa, winogrande, gsm8k, downloads, likes, created_at, embedding)\n",
        "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "            ''', tuple(model_to_insert.values()))\n",
        "\n",
        "        for chunk in list_into_chunks([tuple(readme.values()) for readme in readmes_to_insert]):\n",
        "            with connection.cursor() as cursor:\n",
        "                cursor.executemany(f'''\n",
        "                    INSERT INTO {MODEL_READMES_TABLE_NAME} (model_repo_id, text, created_at, clean_text, embedding)\n",
        "                    VALUES (%s, %s, FROM_UNIXTIME(%s), %s, JSON_ARRAY_PACK(%s))\n",
        "                ''', chunk)\n",
        "    except Exception as e:\n",
        "        print('Error leaderboard_insert_model: ', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3. Create the `leaderboard_process_models` function\n",
        "This function retrieves model data from the LLM Leaderboard dataset that does not exist in the `models` table and inserts those models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.406867Z",
          "iopub.status.busy": "2024-01-10T18:29:18.406681Z",
          "iopub.status.idle": "2024-01-10T18:29:18.417503Z",
          "shell.execute_reply": "2024-01-10T18:29:18.417056Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.406850Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def leaderboard_process_models():\n",
        "    print('Processing models')\n",
        "\n",
        "    existed_model_repo_ids = [i[0] for i in get_models('repo_id', as_dict=False)]\n",
        "    leaderboard_df = leaderboard_get_df()\n",
        "\n",
        "    for i, row in leaderboard_df.iterrows():\n",
        "        if not row['repo_id'] in existed_model_repo_ids:\n",
        "            leaderboard_insert_model(row.to_dict())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Create GitHub functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1. Create the `github_search_repos` function\n",
        "This function search for GitHub repositories based on the keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def github_search_repos(keyword: str, last_created_at):\n",
        "    repos = []\n",
        "    headers = {'Authorization': f'token {GITHUB_ACCESS_TOKEN}'}\n",
        "    query = f'\"{keyword}\" in:name,description,readme'\n",
        "\n",
        "    if last_created_at:\n",
        "        query += f' created:>{last_created_at}'\n",
        "\n",
        "    try:\n",
        "        repos_response = requests.get(\n",
        "            \"https://api.github.com/search/repositories\",\n",
        "            headers=headers,\n",
        "            params={'q': query}\n",
        "        )\n",
        "\n",
        "        if repos_response.status_code == 403:\n",
        "            # Handle rate limiting\n",
        "            rate_limit = repos_response.headers['X-RateLimit-Reset']\n",
        "            if not rate_limit:\n",
        "                return repos\n",
        "\n",
        "            sleep_time = int(rate_limit) - int(time())\n",
        "            if sleep_time > 0:\n",
        "                print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds.\")\n",
        "            sleep(sleep_time)\n",
        "            return github_search_repos(keyword, last_created_at)\n",
        "\n",
        "        if repos_response.status_code != 200:\n",
        "            return repos\n",
        "\n",
        "        for repo in repos_response.json().get('items', []):\n",
        "            try:\n",
        "                readme_response = requests.get(repo['contents_url'].replace('{+path}', 'README.md'), headers=headers)\n",
        "                if repos_response.status_code != 200:\n",
        "                    continue\n",
        "\n",
        "                readme_file = readme_response.json()\n",
        "                if readme_file['size'] > 7000:\n",
        "                    continue\n",
        "\n",
        "                readme_text = requests.get(readme_file['download_url']).text\n",
        "                if not readme_text:\n",
        "                    continue\n",
        "\n",
        "                repos.append({\n",
        "                    'repo_id': repo['id'],\n",
        "                    'name': repo['name'],\n",
        "                    'link': repo['html_url'],\n",
        "                    'created_at': datetime.strptime(repo['created_at'], '%Y-%m-%dT%H:%M:%SZ').timestamp(),\n",
        "                    'description': repo.get('description', ''),\n",
        "                    'readme': readme_text,\n",
        "                })\n",
        "            except:\n",
        "                continue\n",
        "    except:\n",
        "        return repos\n",
        "\n",
        "    return repos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2. Create the `github_insert_model_repos` function\n",
        "This function prepares a repository, creates embedding based on the repository readme text and inserts into the `model_github_repos` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def github_insert_model_repos(model_repo_id, repos):\n",
        "    for repo in repos:\n",
        "        try:\n",
        "            values = []\n",
        "            value = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'repo_id': repo['repo_id'],\n",
        "                'name': repo['name'],\n",
        "                'description': repo['description'],\n",
        "                'clean_text': clean_string(repo['readme']),\n",
        "                'link': repo['link'],\n",
        "                'created_at': repo['created_at'],\n",
        "            }\n",
        "\n",
        "            to_embedding = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'name': value['name'],\n",
        "                'description': value['description'],\n",
        "                'clean_text': value['clean_text']\n",
        "            }\n",
        "\n",
        "            if count_tokens(value['clean_text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
        "                embedding = str(create_embedding(json.dumps(to_embedding)))\n",
        "                values.append({**value, 'embedding': embedding})\n",
        "            else:\n",
        "                for chunk in string_into_chunks(value['clean_text']):\n",
        "                    embedding = str(create_embedding(json.dumps({\n",
        "                        **to_embedding,\n",
        "                        'clean_text': chunk\n",
        "                    })))\n",
        "                    values.append({**value, 'clean_text': chunk, 'embedding': embedding})\n",
        "\n",
        "            for chunk in list_into_chunks([list(value.values()) for value in values]):\n",
        "                with connection.cursor() as cursor:\n",
        "                    cursor.executemany(f'''\n",
        "                        INSERT INTO {MODEL_GITHUB_REPOS_TABLE_NAME} (model_repo_id, repo_id, name, description, clean_text, link, created_at, embedding)\n",
        "                        VALUES (%s, %s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "                    ''', chunk)\n",
        "        except Exception as e:\n",
        "            print('Error github_insert_model_repos: ', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3. Create the `github_process_models_repos` function\n",
        "This function looks for all GitHub repositories that are newer than the latest `created_at` value from `model_github_repos` for each model, and inserts the found repositories into `model_github_repos`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.419736Z",
          "iopub.status.busy": "2024-01-10T18:29:18.419598Z",
          "iopub.status.idle": "2024-01-10T18:29:18.428621Z",
          "shell.execute_reply": "2024-01-10T18:29:18.428122Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.419725Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def github_process_models_repos(existed_models):\n",
        "    print('Processing GitHub posts')\n",
        "\n",
        "    for model in existed_models:\n",
        "        try:\n",
        "            repo_id = model['repo_id']\n",
        "            last_created_at = db_get_last_created_at(MODEL_GITHUB_REPOS_TABLE_NAME, repo_id, True)\n",
        "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
        "            found_repos = github_search_repos(keyword, last_created_at)\n",
        "\n",
        "            if len(found_repos):\n",
        "                github_insert_model_repos(repo_id, found_repos)\n",
        "        except Exception as e:\n",
        "            print('Error github_process_models_repos: ', e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "language": "python"
      },
      "source": [
        "## 10. Create a Twitter client and functions\n",
        "\n",
        "### 10.1. Create a Twitter client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "twitter = tweepy.Client(TWITTER_BEARER_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2. Create the `twitter_search_posts` function\n",
        "This function search for Twitter posts based on the keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def twitter_search_posts(keyword, last_created_at):\n",
        "    posts = []\n",
        "\n",
        "    try:\n",
        "        tweets = twitter.search_recent_tweets(\n",
        "            query=f'{keyword} -is:retweet',\n",
        "            tweet_fields=['id', 'text', 'created_at'],\n",
        "            start_time=last_created_at,\n",
        "            max_results=10\n",
        "        )\n",
        "\n",
        "        for tweet in tweets.data:\n",
        "            posts.append({\n",
        "                'post_id': tweet.id,\n",
        "                'text': tweet.text,\n",
        "                'created_at': tweet.created_at,\n",
        "            })\n",
        "    except Exception:\n",
        "        return posts\n",
        "\n",
        "    return posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3. Create the `twitter_insert_model_posts` function\n",
        "This function prepares a Twitter post, creates embedding based on the post text and inserts into the `model_twitter_posts` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def twitter_insert_model_posts(model_repo_id, posts):\n",
        "    for post in posts:\n",
        "        try:\n",
        "            values = []\n",
        "\n",
        "            value = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'post_id': post['post_id'],\n",
        "                'clean_text': clean_string(post['text']),\n",
        "                'created_at': post['created_at'],\n",
        "            }\n",
        "\n",
        "            to_embedding = {\n",
        "                'model_repo_id': value['model_repo_id'],\n",
        "                'clean_text': value['clean_text']\n",
        "            }\n",
        "\n",
        "            embedding = str(create_embedding(json.dumps(to_embedding)))\n",
        "            values.append({**value, 'embedding': embedding})\n",
        "\n",
        "            for chunk in list_into_chunks([list(value.values()) for value in values]):\n",
        "                with connection.cursor() as cursor:\n",
        "                    cursor.executemany(f'''\n",
        "                        INSERT INTO {MODEL_TWITTER_POSTS_TABLE_NAME} (model_repo_id, post_id, clean_text, created_at, embedding)\n",
        "                        VALUES (%s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "                    ''', chunk)\n",
        "        except Exception as e:\n",
        "            print('Error twitter_insert_model_posts: ', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.4. Create the `twitter_process_models_posts` function\n",
        "This function looks for all Twitter posts that are newer than the latest `created_at` value from `model_twitter_posts` for each model, and inserts the found posts into `model_twitter_posts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.429701Z",
          "iopub.status.busy": "2024-01-10T18:29:18.429398Z",
          "iopub.status.idle": "2024-01-10T18:29:18.436995Z",
          "shell.execute_reply": "2024-01-10T18:29:18.436448Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.429683Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def twitter_process_models_posts(existed_models):\n",
        "    print('Processing Twitter posts')\n",
        "\n",
        "    for model in existed_models:\n",
        "        try:\n",
        "            repo_id = model['repo_id']\n",
        "            last_created_at = db_get_last_created_at(MODEL_TWITTER_POSTS_TABLE_NAME, repo_id, True)\n",
        "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
        "            found_posts = twitter_search_posts(keyword, last_created_at)\n",
        "\n",
        "            if len(found_posts):\n",
        "                twitter_insert_model_posts(repo_id, found_posts)\n",
        "        except Exception as e:\n",
        "            print('Error twitter_process_models_posts: ', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Create a Reddit client and functions\n",
        "\n",
        "### 11.1. Create a Twitter client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reddit = praw.Reddit(\n",
        "    username=REDDIT_USERNAME,\n",
        "    password=REDDIT_PASSWORD,\n",
        "    client_id=REDDIT_CLIENT_ID,\n",
        "    client_secret=REDDIT_CLIENT_SECRET,\n",
        "    user_agent=REDDIT_USER_AGENT\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2. Create the `reddit_search_posts` function\n",
        "This function search for Reddit posts based on the keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reddit_search_posts(keyword: str, last_created_at):\n",
        "    posts = []\n",
        "\n",
        "    try:\n",
        "        for post in reddit.subreddit('all').search(\n",
        "            f'\"{keyword}\"', sort='relevance', time_filter='year', limit=100\n",
        "        ):\n",
        "            contains_keyword = keyword in post.title or keyword in post.selftext\n",
        "\n",
        "            if contains_keyword and not post.over_18:\n",
        "                if not last_created_at or (post.created_utc > last_created_at):\n",
        "                    posts.append({\n",
        "                        'post_id': post.id,\n",
        "                        'title': post.title,\n",
        "                        'text': post.selftext,\n",
        "                        'link': f'https://www.reddit.com{post.permalink}',\n",
        "                        'created_at': post.created_utc,\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print('Error reddit_search_posts: ', e)\n",
        "        return posts\n",
        "\n",
        "    return posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.3. Create the `reddit_insert_model_posts` function\n",
        "This function prepares a Reddit post, creates embedding based on the post text and inserts into the `model_reddit_posts` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.439027Z",
          "iopub.status.busy": "2024-01-10T18:29:18.438838Z",
          "iopub.status.idle": "2024-01-10T18:29:18.448968Z",
          "shell.execute_reply": "2024-01-10T18:29:18.448517Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.439009Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def reddit_insert_model_posts(model_repo_id, posts):\n",
        "    for post in posts:\n",
        "        try:\n",
        "            values = []\n",
        "\n",
        "            value = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'post_id': post['post_id'],\n",
        "                'title': post['title'],\n",
        "                'clean_text': clean_string(post['text']),\n",
        "                'link': post['link'],\n",
        "                'created_at': post['created_at'],\n",
        "            }\n",
        "\n",
        "            to_embedding = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'title': value['title'],\n",
        "                'clean_text': value['clean_text']\n",
        "            }\n",
        "\n",
        "            if count_tokens(value['clean_text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
        "                embedding = str(create_embedding(json.dumps(to_embedding)))\n",
        "                values.append({**value, 'embedding': embedding})\n",
        "            else:\n",
        "                for chunk in string_into_chunks(value['clean_text']):\n",
        "                    embedding = str(create_embedding(json.dumps({\n",
        "                        **to_embedding,\n",
        "                        'clean_text': chunk\n",
        "                    })))\n",
        "                    values.append({**value, 'clean_text': chunk, 'embedding': embedding})\n",
        "\n",
        "            for chunk in list_into_chunks([list(value.values()) for value in values]):\n",
        "                with connection.cursor() as cursor:\n",
        "                    cursor.executemany(f'''\n",
        "                        INSERT INTO {MODEL_REDDIT_POSTS_TABLE_NAME} (model_repo_id, post_id, title, clean_text, link, created_at, embedding)\n",
        "                        VALUES (%s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "                    ''', chunk)\n",
        "        except Exception as e:\n",
        "            print('Error reddit_insert_model_posts: ', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.4. Create the `reddit_process_models_posts` function\n",
        "This function looks for all Reddit posts that are newer than the latest `created_at` value from `model_reddit_posts` for each model, and inserts the found posts into `model_reddit_posts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reddit_process_models_posts(existed_models):\n",
        "    print('Processing Reddit posts')\n",
        "\n",
        "    for model in existed_models:\n",
        "        try:\n",
        "            repo_id = model['repo_id']\n",
        "            last_created_at = db_get_last_created_at(MODEL_REDDIT_POSTS_TABLE_NAME, repo_id)\n",
        "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
        "            found_posts = reddit_search_posts(keyword, last_created_at)\n",
        "\n",
        "            if len(found_posts):\n",
        "                reddit_insert_model_posts(repo_id, found_posts)\n",
        "        except Exception as e:\n",
        "            print('Error reddit_process_models_posts: ', e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Run the \n",
        "First, the notebook creates tables in the database if they don't exist.\n",
        "Next, the notebook retrieves the specified number of models from the Open LLM Leaderboard dataset, creates embeddings, and enters the data into the `models` and `model_reamdes` tables.\n",
        "Next, it executes a query to retrieve all the models in the database. Based on these models, Twitter posts, Reddit posts, and GitHub repositories are searched, converted into embeddings and inserted into tables.\n",
        "\n",
        "Finally, we get a ready set of data for finding the most appropriate model for any use case using semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.920987Z",
          "iopub.status.busy": "2024-01-10T18:29:18.920459Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_tables()\n",
        "\n",
        "leaderboard_process_models()\n",
        "\n",
        "existed_models = get_models('repo_id, name', f'ORDER BY score DESC LIMIT {MODELS_LIMIT}')\n",
        "\n",
        "twitter_process_models_posts(existed_models)\n",
        "reddit_process_models_posts(existed_models)\n",
        "github_process_models_repos(existed_models)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "singlestore_cell_default_language": "python",
    "singlestore_connection": {
      "connectionID": "470575f4-6bb3-4406-bca6-eefdcb76ed8b",
      "defaultDatabase": "llm_recommender_dev"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
