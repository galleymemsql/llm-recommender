{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:15.978807Z",
          "iopub.status.busy": "2024-01-10T18:29:15.978565Z",
          "iopub.status.idle": "2024-01-10T18:29:17.900211Z",
          "shell.execute_reply": "2024-01-10T18:29:17.899595Z",
          "shell.execute_reply.started": "2024-01-10T18:29:15.978792Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%pip install singlestoredb openai tiktoken beautifulsoup4 pandas python-dotenv Markdown praw PyGithub tweepy --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:17.902852Z",
          "iopub.status.busy": "2024-01-10T18:29:17.902643Z",
          "iopub.status.idle": "2024-01-10T18:29:17.908368Z",
          "shell.execute_reply": "2024-01-10T18:29:17.907943Z",
          "shell.execute_reply.started": "2024-01-10T18:29:17.902836Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import openai\n",
        "import tiktoken\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import singlestoredb as s2\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "\n",
        "from github import Github\n",
        "from github import Auth\n",
        "import tweepy\n",
        "import praw"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variables setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:17.909544Z",
          "iopub.status.busy": "2024-01-10T18:29:17.909172Z",
          "iopub.status.idle": "2024-01-10T18:29:17.913736Z",
          "shell.execute_reply": "2024-01-10T18:29:17.913230Z",
          "shell.execute_reply.started": "2024-01-10T18:29:17.909525Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "MODELS_LIMIT = 1000\n",
        "\n",
        "OPENAI_API_KEY = ''\n",
        "\n",
        "HF_TOKEN = ''\n",
        "\n",
        "LEADERBOARD_DATASET_URL = 'https://llm-recommender.vercel.app/datasets/leaderboard.json'\n",
        "\n",
        "REDDIT_USERNAME = ''\n",
        "REDDIT_PASSWORD = ''\n",
        "REDDIT_CLIENT_ID = ''\n",
        "REDDIT_CLIENT_SECRET = ''\n",
        "REDDIT_USER_AGENT = 'llm_recommender_1.0'\n",
        "\n",
        "TWITTER_BEARER_TOKEN=''\n",
        "GITHUB_ACCESS_TOKEN = ''\n",
        "\n",
        "TOKENS_LIMIT = 2047\n",
        "TOKENS_TRASHHOLD_LIMIT = TOKENS_LIMIT - 128\n",
        "\n",
        "MODELS_TABLE_NAME = 'models'\n",
        "MODEL_READMES_TABLE_NAME = 'model_readmes'\n",
        "MODEL_TWITTER_POSTS_TABLE_NAME = 'model_twitter_posts'\n",
        "MODEL_REDDIT_POSTS_TABLE_NAME = 'model_reddit_posts'\n",
        "MODEL_GITHUB_REPOS_TABLE_NAME = 'model_github_repos'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Database setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:17.915576Z",
          "iopub.status.busy": "2024-01-10T18:29:17.915203Z",
          "iopub.status.idle": "2024-01-10T18:29:18.388970Z",
          "shell.execute_reply": "2024-01-10T18:29:18.388047Z",
          "shell.execute_reply.started": "2024-01-10T18:29:17.915558Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "connection = s2.connect(connection_url)\n",
        "\n",
        "\n",
        "def create_tables():\n",
        "    def create_models_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODELS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    name VARCHAR(512) NOT NULL,\n",
        "                    author VARCHAR(512) NOT NULL,\n",
        "                    repo_id VARCHAR(1024) NOT NULL,\n",
        "                    score DECIMAL(5, 2) NOT NULL,\n",
        "                    arc DECIMAL(5, 2) NOT NULL,\n",
        "                    hellaswag DECIMAL(5, 2) NOT NULL,\n",
        "                    mmlu DECIMAL(5, 2) NOT NULL,\n",
        "                    truthfulqa DECIMAL(5, 2) NOT NULL,\n",
        "                    winogrande DECIMAL(5, 2) NOT NULL,\n",
        "                    gsm8k DECIMAL(5, 2) NOT NULL,\n",
        "                    link VARCHAR(255) NOT NULL,\n",
        "                    downloads INT,\n",
        "                    likes INT,\n",
        "                    still_on_hub BOOLEAN NOT NULL,\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_readmes_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_READMES_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_twitter_posts_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_TWITTER_POSTS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    post_id VARCHAR(256),\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_reddit_posts_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_REDDIT_POSTS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    post_id VARCHAR(256),\n",
        "                    title VARCHAR(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    link VARCHAR(256),\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    def create_model_github_repos_table():\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                CREATE TABLE IF NOT EXISTS {MODEL_GITHUB_REPOS_TABLE_NAME} (\n",
        "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "                    model_repo_id VARCHAR(512),\n",
        "                    repo_id INT,\n",
        "                    name VARCHAR(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    description TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    clean_text LONGTEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,\n",
        "                    link VARCHAR(256),\n",
        "                    created_at TIMESTAMP,\n",
        "                    embedding BLOB\n",
        "                )\n",
        "            ''')\n",
        "\n",
        "    create_models_table()\n",
        "    create_model_readmes_table()\n",
        "    create_model_twitter_posts_table()\n",
        "    create_model_reddit_posts_table()\n",
        "    create_model_github_repos_table()\n",
        "\n",
        "\n",
        "def drop_table(table_name: str):\n",
        "    with connection.cursor() as cursor:\n",
        "        cursor.execute(f'DROP TABLE IF EXISTS {table_name}')\n",
        "\n",
        "\n",
        "def get_models(select='*', query='', as_dict=True):\n",
        "    with connection.cursor() as cursor:\n",
        "        _query = f'SELECT {select} FROM {MODELS_TABLE_NAME}'\n",
        "\n",
        "        if query:\n",
        "            _query += f' {query}'\n",
        "\n",
        "        cursor.execute(_query)\n",
        "\n",
        "        if as_dict:\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            return [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
        "\n",
        "        return cursor.fetchall()\n",
        "\n",
        "\n",
        "def db_get_last_created_at(table, repo_id, to_string=False):\n",
        "    with connection.cursor() as cursor:\n",
        "        cursor.execute(f\"\"\"\n",
        "            SELECT UNIX_TIMESTAMP(created_at) FROM {table}\n",
        "            WHERE model_repo_id = '{repo_id}'\n",
        "            ORDER BY created_at DESC\n",
        "            LIMIT 1\n",
        "        \"\"\")\n",
        "\n",
        "        rows = cursor.fetchone()\n",
        "        created_at = float(rows[0]) if rows and rows[0] else None\n",
        "\n",
        "        if (created_at and to_string):\n",
        "            created_at = datetime.fromtimestamp(created_at)\n",
        "            created_at = created_at.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "        return created_at"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.390609Z",
          "iopub.status.busy": "2024-01-10T18:29:18.390411Z",
          "iopub.status.idle": "2024-01-10T18:29:18.398027Z",
          "shell.execute_reply": "2024-01-10T18:29:18.397575Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.390592Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "openapi_key = OPENAI_API_KEY\n",
        "\n",
        "def count_tokens(text: str):\n",
        "    enc = tiktoken.get_encoding('cl100k_base')\n",
        "    return len(enc.encode(text, disallowed_special={}))\n",
        "\n",
        "\n",
        "def create_embedding(input):\n",
        "    try:\n",
        "        data = openai.create(input=input, model='text-embedding-ada-002').data\n",
        "        return data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return [[]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.399019Z",
          "iopub.status.busy": "2024-01-10T18:29:18.398833Z",
          "iopub.status.idle": "2024-01-10T18:29:18.405786Z",
          "shell.execute_reply": "2024-01-10T18:29:18.405275Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.399001Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class JSONEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, datetime):\n",
        "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        return super().default(obj)\n",
        "\n",
        "def list_into_chunks(lst, chunk_size = 100):\n",
        "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
        "\n",
        "\n",
        "def string_into_chunks(string: str, max_tokens=TOKENS_LIMIT):\n",
        "    if count_tokens(string) <= max_tokens:\n",
        "        return [string]\n",
        "\n",
        "    delimiter = ' '\n",
        "    words = string.split(delimiter)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        if count_tokens(delimiter.join(current_chunk + [word])) <= max_tokens:\n",
        "            current_chunk.append(word)\n",
        "        else:\n",
        "            chunks.append(delimiter.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(delimiter.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def clean_string(string: str):\n",
        "    def strip_html_elements(string: str):\n",
        "        html = markdown(string)\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = soup.get_text()\n",
        "        return text.strip()\n",
        "\n",
        "    def remove_unicode_escapes(string: str):\n",
        "        return re.sub(r'[^\\x00-\\x7F]+', '', string)\n",
        "\n",
        "    def remove_string_spaces(strgin: str):\n",
        "        new_string = re.sub(r'\\n+', '\\n', strgin)\n",
        "        new_string = re.sub(r'\\s+', ' ', new_string)\n",
        "        return new_string\n",
        "\n",
        "    def remove_links(string: str):\n",
        "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        return re.sub(url_pattern, '', string)\n",
        "\n",
        "    new_string = strip_html_elements(string)\n",
        "    new_string = remove_unicode_escapes(new_string)\n",
        "    new_string = remove_string_spaces(new_string)\n",
        "    new_string = re.sub(r'\\*\\*+', '*', new_string)\n",
        "    new_string = re.sub(r'--+', '-', new_string)\n",
        "    new_string = re.sub(r'====+', '=', new_string)\n",
        "    new_string = remove_links(new_string)\n",
        "\n",
        "    return new_string"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leaderboard setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.406867Z",
          "iopub.status.busy": "2024-01-10T18:29:18.406681Z",
          "iopub.status.idle": "2024-01-10T18:29:18.417503Z",
          "shell.execute_reply": "2024-01-10T18:29:18.417056Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.406850Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def leaderboard_get_df():\n",
        "    response = requests.get(LEADERBOARD_DATASET_URL)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = json.loads(response.text)\n",
        "        df = pd.DataFrame(data).head(MODELS_LIMIT)\n",
        "        return df\n",
        "    else:\n",
        "        print(\"Failed to retrieve JSON file\")\n",
        "\n",
        "\n",
        "def leaderboard_insert_model(model):\n",
        "    try:\n",
        "        _model = {key: value for key, value in model.items() if key != 'readme'}\n",
        "        to_embedding = json.dumps(_model, cls=JSONEncoder)\n",
        "        embedding = str(create_embedding(to_embedding))\n",
        "        model_to_insert = {**_model, embedding: embedding}\n",
        "        readmes_to_insert = []\n",
        "\n",
        "        if model['readme']:\n",
        "            readme = {\n",
        "                'model_repo_id': model['repo_id'],\n",
        "                'text': model['readme'],\n",
        "                'created_at': time()\n",
        "            }\n",
        "\n",
        "            if count_tokens(readme['text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
        "                readme['clean_text'] = clean_string(readme['text'])\n",
        "                to_embedding = json.dumps({\n",
        "                    'model_repo_id': readme['model_repo_id'],\n",
        "                    'clean_text': readme['clean_text'],\n",
        "                })\n",
        "                readme['embedding'] = str(create_embedding(to_embedding))\n",
        "                readmes_to_insert.append(readme)\n",
        "            else:\n",
        "                for i, chunk in enumerate(string_into_chunks(readme['text'])):\n",
        "                    _readme = {\n",
        "                        **readme,\n",
        "                        'text': chunk,\n",
        "                        'created_at': time()\n",
        "                    }\n",
        "\n",
        "                    _readme['clean_text'] = clean_string(chunk)\n",
        "                    to_embedding = json.dumps({\n",
        "                        'model_repo_id': _readme['model_repo_id'],\n",
        "                        'clean_text': chunk,\n",
        "                    })\n",
        "                    _readme['embedding'] = str(create_embedding(to_embedding))\n",
        "                    readmes_to_insert.append(_readme)\n",
        "\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(f'''\n",
        "                INSERT INTO {MODELS_TABLE_NAME} (name, author, repo_id, score, link, still_on_hub, arc, hellaswag, mmlu, truthfulqa, winogrande, gsm8k, downloads, likes, created_at, embedding)\n",
        "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "            ''', tuple(model_to_insert.values()))\n",
        "\n",
        "        for chunk in list_into_chunks([tuple(readme.values()) for readme in readmes_to_insert]):\n",
        "            with connection.cursor() as cursor:\n",
        "                cursor.executemany(f'''\n",
        "                    INSERT INTO {MODEL_READMES_TABLE_NAME} (model_repo_id, text, created_at, clean_text, embedding)\n",
        "                    VALUES (%s, %s, FROM_UNIXTIME(%s), %s, JSON_ARRAY_PACK(%s))\n",
        "                ''', chunk)\n",
        "    except Exception as e:\n",
        "        print('Error leaderboard_insert_model: ', e)\n",
        "\n",
        "\n",
        "def leaderboard_process_models():\n",
        "    print('Processing models')\n",
        "\n",
        "    existed_model_repo_ids = [i[0] for i in get_models('repo_id', as_dict=False)]\n",
        "    leaderboard_df = leaderboard_get_df()\n",
        "\n",
        "    for i, row in leaderboard_df.iterrows():\n",
        "        if not row['repo_id'] in existed_model_repo_ids:\n",
        "            leaderboard_insert_model(row.to_dict())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GitHub setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.419736Z",
          "iopub.status.busy": "2024-01-10T18:29:18.419598Z",
          "iopub.status.idle": "2024-01-10T18:29:18.428621Z",
          "shell.execute_reply": "2024-01-10T18:29:18.428122Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.419725Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "github = Github(auth=Auth.Token(GITHUB_ACCESS_TOKEN))\n",
        "\n",
        "\n",
        "def github_search_repos(keyword: str, last_created_at):\n",
        "    repos = []\n",
        "    query = f'\"{keyword}\" in:name,description,readme'\n",
        "\n",
        "    if last_created_at:\n",
        "        query += f' created:>{last_created_at}'\n",
        "\n",
        "    try:\n",
        "        for repo in github.search_repositories(query):\n",
        "            try:\n",
        "                readme_file = repo.get_readme()\n",
        "\n",
        "                if readme_file.size > 7000:\n",
        "                    continue\n",
        "\n",
        "                readme = readme_file.decoded_content.decode('utf-8')\n",
        "\n",
        "                repos.append({\n",
        "                    'repo_id': repo.id,\n",
        "                    'name': repo.name,\n",
        "                    'link': repo.html_url,\n",
        "                    'created_at': repo.created_at.timestamp(),\n",
        "                    'description': repo.description if bool(repo.description) else '',\n",
        "                    'readme': readme,\n",
        "                })\n",
        "            except Exception:\n",
        "                continue\n",
        "    except Exception:\n",
        "        return repos\n",
        "\n",
        "    return repos\n",
        "\n",
        "\n",
        "def github_insert_model_repos(model_repo_id, repos):\n",
        "    for repo in repos:\n",
        "        try:\n",
        "            values = []\n",
        "            value = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'repo_id': repo['repo_id'],\n",
        "                'name': repo['name'],\n",
        "                'description': repo['description'],\n",
        "                'clean_text': clean_string(repo['readme']),\n",
        "                'link': repo['link'],\n",
        "                'created_at': repo['created_at'],\n",
        "            }\n",
        "\n",
        "            to_embedding = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'name': value['name'],\n",
        "                'description': value['description'],\n",
        "                'clean_text': value['clean_text']\n",
        "            }\n",
        "\n",
        "            if count_tokens(value['clean_text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
        "                embedding = str(create_embedding(json.dumps(to_embedding)))\n",
        "                values.append({**value, 'embedding': embedding})\n",
        "            else:\n",
        "                for chunk in string_into_chunks(value['clean_text']):\n",
        "                    embedding = str(create_embedding(json.dumps({\n",
        "                        **to_embedding,\n",
        "                        'clean_text': chunk\n",
        "                    })))\n",
        "                    values.append({**value, 'clean_text': chunk, 'embedding': embedding})\n",
        "\n",
        "            for chunk in list_into_chunks([list(value.values()) for value in values]):\n",
        "                with connection.cursor() as cursor:\n",
        "                    cursor.executemany(f'''\n",
        "                        INSERT INTO {MODEL_GITHUB_REPOS_TABLE_NAME} (model_repo_id, repo_id, name, description, clean_text, link, created_at, embedding)\n",
        "                        VALUES (%s, %s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "                    ''', chunk)\n",
        "        except Exception as e:\n",
        "            print('Error github_insert_model_repos: ', e)\n",
        "\n",
        "\n",
        "def github_process_models_repos(existed_models):\n",
        "    print('Processing GitHub posts')\n",
        "\n",
        "    for model in existed_models:\n",
        "        try:\n",
        "            repo_id = model['repo_id']\n",
        "            last_created_at = db_get_last_created_at(MODEL_GITHUB_REPOS_TABLE_NAME, repo_id, True)\n",
        "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
        "            found_repos = github_search_repos(keyword, last_created_at)\n",
        "\n",
        "            if len(found_repos):\n",
        "                github_insert_model_repos(repo_id, found_repos)\n",
        "        except Exception as e:\n",
        "            print('Error github_process_models_repos: ', e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "language": "python"
      },
      "source": [
        "## Twitter setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.429701Z",
          "iopub.status.busy": "2024-01-10T18:29:18.429398Z",
          "iopub.status.idle": "2024-01-10T18:29:18.436995Z",
          "shell.execute_reply": "2024-01-10T18:29:18.436448Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.429683Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "twitter = tweepy.Client(TWITTER_BEARER_TOKEN)\n",
        "\n",
        "\n",
        "def twitter_search_posts(keyword, last_created_at):\n",
        "    posts = []\n",
        "\n",
        "    try:\n",
        "        tweets = twitter.search_recent_tweets(\n",
        "            query=f'{keyword} -is:retweet',\n",
        "            tweet_fields=['id', 'text', 'created_at'],\n",
        "            start_time=last_created_at,\n",
        "            max_results=10\n",
        "        )\n",
        "\n",
        "        for tweet in tweets.data:\n",
        "            posts.append({\n",
        "                'post_id': tweet.id,\n",
        "                'text': tweet.text,\n",
        "                'created_at': tweet.created_at,\n",
        "            })\n",
        "    except Exception:\n",
        "        return posts\n",
        "\n",
        "    return posts\n",
        "\n",
        "\n",
        "def twitter_insert_model_posts(model_repo_id, posts):\n",
        "    for post in posts:\n",
        "        try:\n",
        "            values = []\n",
        "\n",
        "            value = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'post_id': post['post_id'],\n",
        "                'clean_text': clean_string(post['text']),\n",
        "                'created_at': post['created_at'],\n",
        "            }\n",
        "\n",
        "            to_embedding = {\n",
        "                'model_repo_id': value['model_repo_id'],\n",
        "                'clean_text': value['clean_text']\n",
        "            }\n",
        "\n",
        "            embedding = str(create_embedding(json.dumps(to_embedding)))\n",
        "            values.append({**value, 'embedding': embedding})\n",
        "\n",
        "            for chunk in list_into_chunks([list(value.values()) for value in values]):\n",
        "                with connection.cursor() as cursor:\n",
        "                    cursor.executemany(f'''\n",
        "                        INSERT INTO {MODEL_TWITTER_POSTS_TABLE_NAME} (model_repo_id, post_id, clean_text, created_at, embedding)\n",
        "                        VALUES (%s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "                    ''', chunk)\n",
        "        except Exception as e:\n",
        "            print('Error twitter_insert_model_posts: ', e)\n",
        "\n",
        "\n",
        "def twitter_process_models_posts(existed_models):\n",
        "    print('Processing Twitter posts')\n",
        "\n",
        "    for model in existed_models:\n",
        "        try:\n",
        "            repo_id = model['repo_id']\n",
        "            last_created_at = db_get_last_created_at(MODEL_TWITTER_POSTS_TABLE_NAME, repo_id, True)\n",
        "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
        "            found_posts = twitter_search_posts(keyword, last_created_at)\n",
        "\n",
        "            if len(found_posts):\n",
        "                twitter_insert_model_posts(repo_id, found_posts)\n",
        "        except Exception as e:\n",
        "            print('Error twitter_process_models_posts: ', e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reddit setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.439027Z",
          "iopub.status.busy": "2024-01-10T18:29:18.438838Z",
          "iopub.status.idle": "2024-01-10T18:29:18.448968Z",
          "shell.execute_reply": "2024-01-10T18:29:18.448517Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.439009Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# https://www.reddit.com/prefs/apps\n",
        "reddit = praw.Reddit(\n",
        "    username=REDDIT_USERNAME,\n",
        "    password=REDDIT_PASSWORD,\n",
        "    client_id=REDDIT_CLIENT_ID,\n",
        "    client_secret=REDDIT_CLIENT_SECRET,\n",
        "    user_agent=REDDIT_USER_AGENT\n",
        ")\n",
        "\n",
        "\n",
        "def reddit_search_posts(keyword: str, last_created_at):\n",
        "    posts = []\n",
        "\n",
        "    # https://www.reddit.com/dev/api/#GET_search\n",
        "    # https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html#praw.models.Subreddit.search\n",
        "    try:\n",
        "        for post in reddit.subreddit('all').search(\n",
        "            f'\"{keyword}\"', sort='relevance', time_filter='year', limit=100\n",
        "        ):\n",
        "            contains_keyword = keyword in post.title or keyword in post.selftext\n",
        "\n",
        "            if contains_keyword and not post.over_18:\n",
        "                if not last_created_at or (post.created_utc > last_created_at):\n",
        "                    posts.append({\n",
        "                        'post_id': post.id,\n",
        "                        'title': post.title,\n",
        "                        'text': post.selftext,\n",
        "                        'link': f'https://www.reddit.com{post.permalink}',\n",
        "                        'created_at': post.created_utc,\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print('Error reddit_search_posts: ', e)\n",
        "        return posts\n",
        "\n",
        "    return posts\n",
        "\n",
        "\n",
        "def reddit_insert_model_posts(model_repo_id, posts):\n",
        "    for post in posts:\n",
        "        try:\n",
        "            values = []\n",
        "\n",
        "            value = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'post_id': post['post_id'],\n",
        "                'title': post['title'],\n",
        "                'clean_text': clean_string(post['text']),\n",
        "                'link': post['link'],\n",
        "                'created_at': post['created_at'],\n",
        "            }\n",
        "\n",
        "            to_embedding = {\n",
        "                'model_repo_id': model_repo_id,\n",
        "                'title': value['title'],\n",
        "                'clean_text': value['clean_text']\n",
        "            }\n",
        "\n",
        "            if count_tokens(value['clean_text']) <= TOKENS_TRASHHOLD_LIMIT:\n",
        "                embedding = str(create_embedding(json.dumps(to_embedding)))\n",
        "                values.append({**value, 'embedding': embedding})\n",
        "            else:\n",
        "                for chunk in string_into_chunks(value['clean_text']):\n",
        "                    embedding = str(create_embedding(json.dumps({\n",
        "                        **to_embedding,\n",
        "                        'clean_text': chunk\n",
        "                    })))\n",
        "                    values.append({**value, 'clean_text': chunk, 'embedding': embedding})\n",
        "\n",
        "            for chunk in list_into_chunks([list(value.values()) for value in values]):\n",
        "                with connection.cursor() as cursor:\n",
        "                    cursor.executemany(f'''\n",
        "                        INSERT INTO {MODEL_REDDIT_POSTS_TABLE_NAME} (model_repo_id, post_id, title, clean_text, link, created_at, embedding)\n",
        "                        VALUES (%s, %s, %s, %s, %s, FROM_UNIXTIME(%s), JSON_ARRAY_PACK(%s))\n",
        "                    ''', chunk)\n",
        "        except Exception as e:\n",
        "            print('Error reddit_insert_model_posts: ', e)\n",
        "\n",
        "\n",
        "def reddit_process_models_posts(existed_models):\n",
        "    print('Processing Reddit posts')\n",
        "\n",
        "    for model in existed_models:\n",
        "        try:\n",
        "            repo_id = model['repo_id']\n",
        "            last_created_at = db_get_last_created_at(MODEL_REDDIT_POSTS_TABLE_NAME, repo_id)\n",
        "            keyword = model['name'] if re.search(r'\\d', model['name']) else repo_id\n",
        "            found_posts = reddit_search_posts(keyword, last_created_at)\n",
        "\n",
        "            if len(found_posts):\n",
        "                reddit_insert_model_posts(repo_id, found_posts)\n",
        "        except Exception as e:\n",
        "            print('Error reddit_process_models_posts: ', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.449778Z",
          "iopub.status.busy": "2024-01-10T18:29:18.449596Z",
          "iopub.status.idle": "2024-01-10T18:29:18.919669Z",
          "shell.execute_reply": "2024-01-10T18:29:18.919151Z",
          "shell.execute_reply.started": "2024-01-10T18:29:18.449761Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !DANGER! Uncomment only when you want to drop all tables to reset the data.\n",
        "# DO NOT FORGET TO COMMENT THIS BLOCK AGAIN.\n",
        "# drop_table('models')\n",
        "# drop_table('model_readmes')\n",
        "# drop_table('model_twitter_posts')\n",
        "# drop_table('model_reddit_posts')\n",
        "# drop_table('model_github_repos')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute scheduled logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-10T18:29:18.920987Z",
          "iopub.status.busy": "2024-01-10T18:29:18.920459Z"
        },
        "language": "python",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "create_tables()\n",
        "\n",
        "leaderboard_process_models()\n",
        "\n",
        "existed_models = get_models('repo_id, name', 'ORDER BY score DESC')\n",
        "\n",
        "twitter_process_models_posts(existed_models)\n",
        "reddit_process_models_posts(existed_models)\n",
        "github_process_models_repos(existed_models)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "singlestore_cell_default_language": "python",
    "singlestore_connection": {
      "connectionID": "470575f4-6bb3-4406-bca6-eefdcb76ed8b",
      "defaultDatabase": "llm_recommender_dev"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
